{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PySparkShell').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import operator\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, DateType, TimestampType, ShortType\n",
    "from pyspark.sql.types import IntegerType, LongType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_schema = StructType([\n",
    "    StructField('artistId', LongType(), False),\n",
    "    StructField('artistName', StringType(), True),\n",
    "])\n",
    "\n",
    "chart_artist_mapping_schema = StructType([\n",
    "    StructField('songId', LongType(), False),\n",
    "    StructField('artistId', LongType(), True),\n",
    "])\n",
    "\n",
    "charts_schema = StructType([\n",
    "    StructField('songId', LongType(), False),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('position', ShortType(), True),\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('countryId', LongType(), True),\n",
    "    StructField('chartName', StringType(), True),\n",
    "    StructField('movement', StringType(), True),\n",
    "    StructField('streams', LongType(), True),\n",
    "])\n",
    "\n",
    "regions_schema = StructType([\n",
    "    StructField('countryId', LongType(), False),\n",
    "    StructField('countryName', StringType(), True),\n",
    "])\n",
    "\n",
    "# # Write parquet files\n",
    "# it = [\n",
    "#     ('artists', artists_schema),\n",
    "#     ('chart_artist_mapping', chart_artist_mapping_schema),\n",
    "#     ('charts', charts_schema),\n",
    "#     ('regions', regions_schema),\n",
    "# ]\n",
    "\n",
    "# for stem, schema in it:\n",
    "#     print(stem)\n",
    "    \n",
    "#     df = spark.read.csv(\n",
    "#         'files/{}.csv'.format(stem),\n",
    "#         sep=',', header='false', schema=schema\n",
    "#     )\n",
    "#     df.write.save('files/{}.parquet'.format(stem))\n",
    "    \n",
    "#     spark.read.parquet('files/{}.parquet'.format(stem)).show(2)\n",
    "#     print('=' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(s):\n",
    "    dt = datetime.datetime.strptime(s[:10], '%Y-%m-%d')\n",
    "    return dt.date()\n",
    "\n",
    "\n",
    "def tuple_to_string(t):\n",
    "    return ','.join(str(x) for x in t)\n",
    "\n",
    "\n",
    "def timedSql(query, path):\n",
    "    tStart = time.perf_counter()\n",
    "    spark.sql(query).coalesce(1).write.csv(\n",
    "        path, header=True, sep=',', mode='overwrite'\n",
    "    )\n",
    "    tEnd = time.perf_counter()\n",
    "    elapsed = tEnd - tStart\n",
    "    print('{:.2f} seconds elapsed'.format(elapsed))\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "def timedRdd(rdd, path):\n",
    "    tStart = time.perf_counter()\n",
    "    rdd.map(tuple_to_string).coalesce(1).saveAsTextFile(path)\n",
    "    tEnd = time.perf_counter()\n",
    "    elapsed = tEnd - tStart\n",
    "    print('{:.2f} seconds elapsed'.format(elapsed))\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliases for easier indexing\n",
    "songId = 0\n",
    "title = 1\n",
    "position = 2\n",
    "date = 3\n",
    "countryId = 4\n",
    "chartName = 5\n",
    "movement = 6\n",
    "streams = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = spark.read.parquet('files/artists.parquet')\n",
    "artistsCsv = spark.read.csv(\n",
    "    'files/artists.csv',\n",
    "    sep=',', header='false', schema=artists_schema\n",
    ")\n",
    "artistsRdd = sc.textFile('files/artists.csv').map(lambda line: line.split(','))\n",
    "\n",
    "chart_artist_mapping = spark.read.parquet('files/chart_artist_mapping.parquet')\n",
    "chart_artist_mappingCsv = spark.read.csv(\n",
    "    'files/chart_artist_mapping.csv',\n",
    "    sep=',', header='false', schema=chart_artist_mapping_schema\n",
    ")\n",
    "chart_artist_mappingRdd = (\n",
    "    sc.textFile('files/chart_artist_mapping.csv')\n",
    "      .map(lambda line: line.split(','))\n",
    ")\n",
    "\n",
    "charts = spark.read.parquet('files/charts.parquet')\n",
    "chartsCsv = spark.read.csv(\n",
    "    'files/charts.csv',\n",
    "    sep=',', header='false', schema=charts_schema\n",
    ")\n",
    "chartsRdd = sc.textFile('files/charts.csv').map(lambda line: line.split(','))\n",
    "\n",
    "regions = spark.read.parquet('files/regions.parquet')\n",
    "regionsCsv = spark.read.csv(\n",
    "    'files/regions.csv',\n",
    "    sep=',', header='false', schema=regions_schema\n",
    ")\n",
    "regionsRdd = sc.textFile('files/regions.csv').map(lambda line: line.split(','))\n",
    "\n",
    "\n",
    "def useParquet():\n",
    "    charts.createOrReplaceTempView('charts')\n",
    "    regions.createOrReplaceTempView('regions')\n",
    "    chart_artist_mapping.createOrReplaceTempView('chart_artist_mapping')\n",
    "    artists.createOrReplaceTempView('artists')\n",
    "\n",
    "    \n",
    "def useCsv():\n",
    "    chartsCsv.createOrReplaceTempView('charts')\n",
    "    regionsCsv.createOrReplaceTempView('regions')\n",
    "    chart_artist_mappingCsv.createOrReplaceTempView('chart_artist_mapping')\n",
    "    artistsCsv.createOrReplaceTempView('artists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {'query{}'.format(i): {'parquet': None, 'csv': None, 'rdd': None}\n",
    "         for i in range(1, 7)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = '''\n",
    "    SELECT sum(streams) FROM charts\n",
    "    WHERE chartName=\"top200\" AND title=\"Shape of You\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useParquet()\n",
    "times['query1']['parquet'] = timedSql(query1, 'output/query1_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCsv()\n",
    "times['query1']['csv'] = timedSql(query1, 'output/query1_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "res1 = (\n",
    "    chartsRdd\n",
    "     .filter(lambda x: x[chartName] == 'top200' and x[title] == 'Shape of You')\n",
    "     .map(lambda x: int(x[streams]))\n",
    "     .reduce(operator.add)\n",
    ")\n",
    "t1 = time.perf_counter()\n",
    "times['query1']['rdd'] = t1 - t0\n",
    "sc.parallelize([str(res1)]).coalesce(1).saveAsTextFile('output/query1_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = '''\n",
    "    SELECT chartName, title, avgCount\n",
    "    FROM (\n",
    "        SELECT chartName, title, avgCount,\n",
    "            max(avgCount) OVER (PARTITION BY chartName) AS maxAvgCount\n",
    "        FROM (\n",
    "            SELECT chartName, first(title) title, count(*)/69 avgCount\n",
    "            FROM charts\n",
    "            WHERE position=1\n",
    "            GROUP BY chartName, songId\n",
    "        )\n",
    "    )\n",
    "    WHERE avgCount = maxAvgCount\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useParquet()\n",
    "times['query2']['parquet'] = timedSql(query2, 'output/query2_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCsv()\n",
    "times['query2']['csv'] = timedSql(query2, 'output/query2_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = (\n",
    "    chartsRdd\n",
    "     .filter(lambda x: x[position] == '1')\n",
    "     .map(lambda x: ((x[chartName], x[title]), 1))\n",
    "     .reduceByKey(operator.add)\n",
    "     .map(lambda x: (x[0][0], (x[0][1], x[1]/69)))\n",
    "     .reduceByKey(lambda x, y: x if x[1] > y[1] else y)\n",
    "     .map(lambda x: (x[0], *x[1]))\n",
    ")\n",
    "\n",
    "times['query2']['rdd'] = timedRdd(rdd2, 'output/query2_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = '''\n",
    "    SELECT year(date), month(date), sum(streamsDay) streams1, count(*)\n",
    "    FROM (\n",
    "        SELECT date, sum(streams) streamsDay\n",
    "        FROM charts\n",
    "        WHERE position = 1 AND chartName == \"top200\"\n",
    "        GROUP BY date\n",
    "    )\n",
    "    GROUP BY year(date), month(date)\n",
    "    ORDER BY year(date), month(date)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useParquet()\n",
    "times['query3']['parquet'] = timedSql(query3, 'output/query3_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCsv()\n",
    "times['query3']['csv'] = timedSql(query3, 'output/query3_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = (\n",
    "    chartsRdd\n",
    "     .filter(lambda x: x[position] == '1' and x[chartName] == 'top200')\n",
    "     .map(lambda x: (parse_date(x[date]), int(x[streams])))\n",
    "     .reduceByKey(operator.add)\n",
    "     .map(lambda x: ((x[0].year, x[0].month), (x[1], 1)))\n",
    "     .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "     .sortByKey()\n",
    "     .map(lambda x:(x[0][0], x[0][1], x[1][0] / x[1][1]))\n",
    ")\n",
    "times['query3']['rdd'] = timedRdd(rdd3, 'output/query3_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = '''\n",
    "    SELECT countryName, songId songId, title, maxCount\n",
    "    FROM (\n",
    "        SELECT countryId, songId, title, cnt, \n",
    "            max(cnt) OVER (PARTITION BY countryId) AS maxCount \n",
    "        FROM (\n",
    "            SELECT countryId, songId, first(title) title, count(*) cnt\n",
    "            FROM charts\n",
    "            WHERE chartName = \"viral50\"\n",
    "            GROUP BY countryId, songId\n",
    "        )\n",
    "    ) a\n",
    "    LEFT JOIN regions\n",
    "    ON a.countryId = regions.countryId\n",
    "    WHERE cnt = maxCount\n",
    "    ORDER BY countryName, title\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useParquet()\n",
    "times['query4']['parquet'] = timedSql(query4, 'output/query4_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCsv()\n",
    "times['query4']['csv'] = timedSql(query4, 'output/query4_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countryId, (songId, title, count)\n",
    "counts = (\n",
    "    chartsRdd\n",
    "     .filter(lambda x: x[chartName] == 'viral50')\n",
    "     .map(lambda x: ((x[countryId], x[songId]), (x[title], 1)))\n",
    "     .reduceByKey(lambda x, y: (x[0], x[1] + y[1]))\n",
    "     .map(lambda x: (x[0][0], (x[0][1], *x[1])))\n",
    ")\n",
    "\n",
    "# countryId, maxCount\n",
    "maxCounts = (\n",
    "    counts\n",
    "     .map(lambda x: (x[0], x[1][2]))\n",
    "     .reduceByKey(max)\n",
    ")\n",
    "\n",
    "# Joining now because both tables are tiny and of equal index\n",
    "\n",
    "# countryId, (countryName, maxCount)\n",
    "maxCountsNamed = regionsRdd.join(maxCounts)\n",
    "\n",
    "rdd4 = (\n",
    "    counts\n",
    "     .join(maxCountsNamed)\n",
    "     # countryId, ((songId, title, count), (countryName, maxCount))\n",
    "     .filter(lambda x: x[1][0][2] == x[1][1][1])  # count == maxCount\n",
    "     .map(lambda x: (x[1][1][0], x[1][0][0], x[1][0][1], x[1][1][1]))\n",
    "     # countryName, songId, songName, maxCount\n",
    "     .sortBy(lambda x: (x[0], x[1]))\n",
    "     # countryName, songId, songName, maxCount\n",
    ")\n",
    "\n",
    "times['query4']['rdd'] = timedRdd(rdd4, 'output/query4_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = '''\n",
    "    SELECT yr, artistName, maxAvgStreams\n",
    "    FROM (\n",
    "        SELECT yr, artistId, avgStreams,\n",
    "            max(avgStreams) OVER (PARTITION BY yr) AS maxAvgStreams \n",
    "        FROM (\n",
    "            SELECT year(date) yr, artistId, sum(streams)/69 avgStreams\n",
    "            FROM charts\n",
    "            JOIN chart_artist_mapping\n",
    "            ON charts.songId = chart_artist_mapping.songId\n",
    "            WHERE chartName = \"top200\"\n",
    "            GROUP BY year(date), artistId\n",
    "        )\n",
    "    ) a\n",
    "    JOIN artists\n",
    "    ON a.artistId = artists.artistId\n",
    "    WHERE avgStreams = maxAvgStreams\n",
    "    ORDER BY yr\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useParquet()\n",
    "times['query5']['parquet'] = timedSql(query5, 'output/query5_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCsv()\n",
    "times['query5']['csv'] = timedSql(query5, 'output/query5_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgStreams = (\n",
    "    chartsRdd\n",
    "     .filter(lambda x: x[chartName] == 'top200')\n",
    "     .map(lambda x: (x[songId], (parse_date(x[date]).year, int(x[streams]))))\n",
    "     .join(chart_artist_mappingRdd)\n",
    "     # songId, ((year, streams), artistId)\n",
    "     .map(lambda x: ((x[1][0][0], x[1][1]), x[1][0][1]/69))\n",
    "     # (year, artistId), streams\n",
    "     .reduceByKey(operator.add)\n",
    "     # (year, artistId), avgStreams\n",
    ")\n",
    "\n",
    "maxAvgStreams = (\n",
    "    avgStreams\n",
    "     .map(lambda x: (x[0][0], x[1]))\n",
    "     .reduceByKey(max)\n",
    "     # year, maxAvgStreams\n",
    ")\n",
    "\n",
    "rdd5 = (\n",
    "    avgStreams\n",
    "     .map(lambda x: (x[1], x[0]))\n",
    "     # avgStreams, (year, artistId)\n",
    "     .join(maxAvgStreams.map(lambda x: (x[1], None)))\n",
    "     # avgStreams, ((year, artistId), None)\n",
    "     .map(lambda x: (x[1][0][1], (x[1][0][0], x[0])))\n",
    "     # artistId, (year, maxAvgStreams)\n",
    "     .join(artistsRdd)\n",
    "     # artistId, ((year, maxAvgStreams), artistName)\n",
    "     .map(lambda x: (x[1][0][0], x[1][1], x[1][0][1]))\n",
    "     # (year, artistName, maxAvgStreams)\n",
    "     .sortBy(lambda x: (x[0], x[1]))\n",
    "     # (year, artistName, maxAvgStreams)\n",
    ")\n",
    "\n",
    "times['query5']['rdd'] = timedRdd(rdd5, 'output/query5_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given examples seem to be wrong.\n",
    "\n",
    "```\n",
    "viral50,2017,21 Savage,13\n",
    "viral50,2017,Post Malone,13\n",
    "```\n",
    "For example the song `rockstar` (`songId = 178991`) by `Post Malone` and `21 Savage` does not have `13` consecutive days but `11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proof\n",
    "spark.sql('''\n",
    "    SELECT *\n",
    "    FROM charts\n",
    "    JOIN chart_artist_mapping\n",
    "    ON charts.songId = chart_artist_mapping.songId\n",
    "    JOIN artists\n",
    "    ON chart_artist_mapping.artistId = artists.artistId\n",
    "    WHERE countryId = 23\n",
    "        AND position = 1\n",
    "        AND artists.artistName = '21 Savage'\n",
    "        AND year(date) = 2017\n",
    "        AND chartName = 'viral50'\n",
    "    ORDER BY date\n",
    "''').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query6 = '''\n",
    "    SELECT chartName, yr, artistName, maxStreak\n",
    "    FROM (\n",
    "        SELECT chartName, yr, songId, count(*) streak,\n",
    "            max(count(*)) OVER (PARTITION BY chartName, yr) AS maxStreak\n",
    "        FROM (\n",
    "            SELECT chartName, year(date) yr, songId,\n",
    "                date_sub(\n",
    "                    date,\n",
    "                    dense_rank() OVER (\n",
    "                        PARTITION BY chartName, year(date), songId\n",
    "                        ORDER BY date\n",
    "                    )\n",
    "                ) streakGrp\n",
    "            FROM charts\n",
    "            WHERE countryId = 23 AND position = 1\n",
    "        )\n",
    "        GROUP BY chartName, yr, songId, streakGrp\n",
    "    ) a\n",
    "    JOIN chart_artist_mapping\n",
    "    ON a.songId = chart_artist_mapping.songId\n",
    "    JOIN artists\n",
    "    ON chart_artist_mapping.artistId = artists.artistId\n",
    "    WHERE streak = maxStreak\n",
    "    ORDER BY chartName, yr\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useParquet()\n",
    "times['query6']['parquet'] = timedSql(query6, 'output/query6_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCsv()\n",
    "times['query6']['csv'] = timedSql(query6, 'output/query6_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestStreak(seq):\n",
    "    seq = sorted(seq)\n",
    "    res = 0\n",
    "    streak = 1\n",
    "    for prev, nxt in zip(seq, seq[1:]):\n",
    "        if (nxt - prev).days == 1:\n",
    "            streak += 1\n",
    "        else:\n",
    "            res = max(res, streak)\n",
    "            streak = 1\n",
    "    res = max(res, streak)\n",
    "    return res\n",
    "\n",
    "\n",
    "streaks = (\n",
    "    chartsRdd\n",
    "     .filter(lambda x: x[countryId] == '23' and x[position] == '1')\n",
    "     .map(lambda x: ((x[chartName], parse_date(x[date]).year, x[songId]), (parse_date(x[date]),) ))\n",
    "     # (chartName, year, songId), (date,)\n",
    "     .reduceByKey(operator.add)\n",
    "     # (chartName, year, songId), (date1, date2, ...)\n",
    "     .mapValues(bestStreak)\n",
    "     # (chartName, year, songId), streak\n",
    ")\n",
    "\n",
    "maxStreaks = (\n",
    "    streaks\n",
    "     .map(lambda x: (x[0][:2], x[1]))\n",
    "     # (chartName, year), streak\n",
    "     .reduceByKey(max)\n",
    "     # (chartName, year), maxStreak\n",
    ")\n",
    "\n",
    "rdd6 = (\n",
    "    streaks\n",
    "     # (chartName, year, songId), maxStreak\n",
    "     .map(lambda x: ((x[0][0], x[0][1], x[1]), x[0][2]))\n",
    "     # (chartName, year, maxStreak), songId\n",
    "     .join(maxStreaks.map(lambda x: ((*x[0], x[1]), None)))\n",
    "     # (chartName, year, maxStreak), (songId, None)\n",
    "     .map(lambda x: (x[1][0], x[0]))\n",
    "     # songId, (chartName, year, maxStreak)\n",
    "     .join(chart_artist_mappingRdd)\n",
    "     # songID, ((chartName, year, maxStreak), artistId)\n",
    "     .map(lambda x: (x[1][1], x[1][0]))\n",
    "     # artistId, (chartName, year, maxStreak)\n",
    "     .join(artistsRdd)\n",
    "     # artistId, ((chartName, year, maxStreak), artistName)\n",
    "     .map(lambda x: (x[1][0][0], x[1][0][1], x[1][1], x[1][0][2]))\n",
    "     # chartName, year, artistName, maxStreak\n",
    "     .sortBy(lambda x: x[:2])\n",
    "     # chartName, year, artistName, maxStreak\n",
    ")\n",
    "\n",
    "times['query6']['rdd'] = timedRdd(rdd5, 'output/query6_rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(times, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "df.plot.barh(ax=ax)\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_title('Times')\n",
    "ax.invert_yaxis()\n",
    "plt.savefig('times.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e62cdb2fab5fb240ec6e260358bf63224313d17ebcaacec076aef54bb50eba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
